# LLM-Wiki-Reader
LLM wrapping application that can use an configurable LLM model to process, and use Wiki articles as context for configurable query prompts. (dockerized and CUDA enabled)
